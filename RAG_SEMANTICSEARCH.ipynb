from scapy.all import rdpcap, DNS, DNSRR, IP, TCP, Raw
from sentence_transformers import SentenceTransformer, util
from tabulate import tabulate
from Levenshtein import ratio  
import subprocess
import re
import matplotlib.pyplot as plt
import torch
from collections import Counter
from sklearn.manifold import TSNE
import numpy as np
import openai
import time
import faiss
import warnings

warnings.filterwarnings('ignore')

#Replace with your pcap files
pcap_file_paths = [
        "C:\\Users\\user\\Downloads\\redline.pcap",
        "C:\\Users\\user\\Downloads\\XLoader.pcap",
        "C:\\Users\\user\\Downloads\\qbot.pcap"
    ]


#Add your API Key
openai.api_key="sk- <INSERT YOUR KEY>"

#Defining the transformer Model
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

#Knowledge base for Demo! Add/Edit as per your requirement
knowledge_base = [
    "web request to suspicious top level domains like .top like http://abc.top",
    "web request to suspicious top level domains like .xyz like http://ijk.xyz",
    "web requests containing the download of executable like name ending with .exe",
    "web request to an address like http://100.200.300.400/abcd.dat along file name ending with .dat",
    "web request to .finance domain is legitimate and good one"
]
knowledge_embeddings = embedding_model.encode(knowledge_base, convert_to_tensor=True)


# This section is for LLM FAISS calculations
# Precompute knowledge base embeddings
knowledge_embeddings_f0 = embedding_model.encode(knowledge_base, convert_to_tensor=False)
knowledge_embeddings_f = np.array(knowledge_embeddings_f0).astype('float32')
faiss.normalize_L2(knowledge_embeddings_f)

# Build a FAISS index for the knowledge base
# Dimensionality of embeddings
d = knowledge_embeddings_f.shape[1]

#Whenrever the SEARCH operation is called, it will be on Inner Product.
#With IndexFlatIP, FAISS compares your query to every stored vector using the inner product ‚Äî a mathematical way to measure how closely two embeddings align. 
faiss_index = faiss.IndexFlatIP(d)
faiss_index.add(knowledge_embeddings_f)
#################


#This Section deals with SEMANTIC MATCHINGS (Cosine Similarity Search)
# Added the EDIT DISTANCE for improving accuracy
def detect_anomalies_rag(http_get_requests, http_post_requests):
    """Detect anomalies dynamically without predefined thresholds."""
    anomalies = {}
    http_requests = http_get_requests + http_post_requests
    # Encode incoming requests
    request_embeddings = embedding_model.encode(http_requests, convert_to_tensor=True)

    for i, request in enumerate(http_requests):
        # Compute cosine similarity
        similarities = util.pytorch_cos_sim(request_embeddings[i], knowledge_embeddings)
        max_score, best_match_idx = similarities.max().item(), similarities.argmax().item()
        best_match_text = knowledge_base[best_match_idx]
        text_similarity = ratio(request, best_match_text)

        # Compute Levenshtein distance
        if max_score > 0.40:
             anomalies[request] = f"Flagged as anomalous. Matched with the context in Knowledge_Base: {best_match_text})"
        else: 
        
           if abs(max_score - text_similarity) >= 0.02:
               continue
           # Dynamic threshold logic
           dynamic_threshold = max(.5, max_score * 0.95)

           # Trigger anomaly if cosine similarity + edit distance confirm match
           if max_score >= dynamic_threshold or text_similarity > 0.05:
              if max_score >= 0.24:
                  if text_similarity > 0.19:
                       anomalies[request] = f"Flagged as anomalous. Matched with the context in Knowledge_Base: {best_match_text})"
    
    num_anomalies = len(anomalies)
    print(f"\nTotal Anomalies Detected: {num_anomalies}")
    return anomalies


#This Section is OPTIONAL. This is to Visulize the "Normal Requests and Anomalies using TSNE"
def visualize_embeddings_anomalies(request_embeddings, anomalies):
    tsne = TSNE(n_components=2, perplexity=5, random_state=42)
    reduced_embeddings = tsne.fit_transform(request_embeddings.cpu().numpy())
    
    plt.figure(figsize=(10, 6))
    anomaly_indices = [i for i, req in enumerate(anomalies) if req in anomalies]
    normal_indices = [i for i in range(len(request_embeddings)) if i not in anomaly_indices]
    
    plt.scatter(reduced_embeddings[normal_indices, 0], reduced_embeddings[normal_indices, 1], c='blue', alpha=0.7, label="Normal Requests")
    plt.scatter(reduced_embeddings[anomaly_indices, 0], reduced_embeddings[anomaly_indices, 1], c='red', alpha=0.7, label="Anomalies")
    
    plt.legend()
    plt.title("Normal Requests vs Anomalies")
    plt.show()



#This section is to extract domains/IP from pcap with tshark for comparing it with knowledgebase data
def extract_domain_ip_and_http_requests(pcap_file_path):
    domain_ip_relations = []
    get_requests = []
    post_requests = []
    
    # Initialize counters with default keys
    request_counter = Counter({"GET": 0, "POST": 0, "TOTAL_HTTP": 0, "TOTAL_HTTPS": 0})
    
    # Extract DNS responses for domain-to-IP mapping
    dns_command = [
        "tshark", "-r", pcap_file_path, "-Y", "dns.flags.response == 1", "-T", "fields", 
        "-e", "dns.qry.name", "-e", "dns.a"
    ]
    dns_output = subprocess.run(dns_command, capture_output=True, text=True).stdout
    
    for line in dns_output.splitlines():
        parts = line.split('\t')
        if len(parts) == 2 and parts[0] and parts[1]:
            domain_ip_relations.append((parts[0], parts[1]))
    
    # Extract HTTP and HTTPS requests
    # Ensure that tshark outputs four fields: http.host, http.request.uri, http.request.method, ssl.handshake.extensions_server_name
    http_command = [
        "tshark", "-r", pcap_file_path, "-Y", "http.request || ssl.handshake.extensions_server_name", "-T", "fields", 
        "-e", "http.host", "-e", "http.request.uri", "-e", "http.request.method", "-e", "ssl.handshake.extensions_server_name"
    ]
    http_output = subprocess.run(http_command, capture_output=True, text=True).stdout
    
    for line in http_output.splitlines():
        parts = line.split('\t')
        # Debug: print the raw parts to inspect output
        if len(parts) >= 3:
            host, path, method = parts[:3]
             # Get the server_name if available, and strip any extra whitespace
            if len(parts) >= 4:
                 server_name = parts[3].strip()
                 #print("DEBUG: server_name =", repr(server_name))
            else:
                       server_name = ""
                
            if (host and path) or server_name:
                # Determine scheme based on whether server_name has a non-empty value
                scheme = "https" if server_name != "" else "http"
                full_url = f"{scheme}://{host}{path}"
                
                if method == "GET":
                    get_requests.append(full_url)
                    request_counter["GET"] += 1
                elif method == "POST":
                    post_requests.append(full_url)
                    request_counter["POST"] += 1
                
                if scheme == "https":
                    request_counter["TOTAL_HTTPS"] += 1 
                else:
                    request_counter["TOTAL_HTTP"] += 1
                
    counter_all = request_counter["TOTAL_HTTP"] + request_counter["TOTAL_HTTPS"]
    print ("Total Traffic:",counter_all)
    return domain_ip_relations, get_requests, post_requests


#Display the anomalies in tabular format
def print_anomalies(anomalies):
    """Print detected anomalies in a structured table format."""
    if not anomalies:
        print("‚úÖ No anomalies detected.")
        return

    print("\nüö® Anomalies Detected:\n")
    table_data = [[url, description] for url, description in anomalies.items()]
    print(tabulate(table_data, headers=["üîóURL", "üõ°Ô∏èAnomaly Description"], tablefmt="grid"))


####This section is to Retrieve data based on RAG concepts
### We used FAISS algorithms to fetch k nearest neighbours
## you can adjust the top_k variable as per your choice

def retrieve_context_for_query(query, top_k=2):
    """
    Encode the query and perform a similarity search on the knowledge base using FAISS.
    Returns the top_k most relevant knowledge base entries and their similarity scores.
    """
    # Encode query to get its embedding
    query_embedding = embedding_model.encode(query, convert_to_tensor=True)
    query_embedding = np.array(query_embedding).astype('float32').reshape(1, -1)
    faiss.normalize_L2(query_embedding)
    
    # Search already created FAISS index for knowledge_base
    distances, indices = faiss_index.search(query_embedding, top_k)
    retrieved_docs = [knowledge_base[i] for i in indices[0]]
    return retrieved_docs, distances[0]


#This section is where we input the Context to LLM
#It combines user query + retrieved documents in one single prompt and send it to llm - GPT-4
def analyze_query(query):
    """
    Retrieve context from the knowledge base using FAISS, then combine the context
    with the query to prompt an LLM (GPT-4) for analysis.
    """
    retrieved_docs, scores = retrieve_context_for_query(query, top_k=2)
    context = "\n".join(retrieved_docs)
    
    prompt = (
        f"Context:\n{context}\n\n"
        f"Query:\n{query}\n\n"
        "You are an AI assistant specialized in network security and you interact with a cyber security analyst, provide a detailed analysis of the query "
        "based on the context provided. Explain any potential threat indicators or anomalies and its recommendations if any"
    )
    
    response = openai.ChatCompletion.create(
         model="gpt-4",
         messages=[
             {"role": "system", "content": "You are a network security specialist"},
             {"role": "user", "content": prompt}
         ],
         temperature=0.1,
         max_tokens=1000    
    )
    time.sleep(1)
    analysis = response.choices[0].message['content']
    return analysis, retrieved_docs, scores


#This section deals with the TextBox for handling userinput for interaction with LLM
def query_llm_interface():
    """Provides a query text box (command-line interface) for user interaction with the LLM."""
    print("\n--- LLM Query Interface ---")
    print("Type 'exit' to quit.")
    
    while True:
        user_query = input("Enter your query: ")
        if user_query.lower().strip() == "exit":
            break
        
        analysis, docs, scores = analyze_query(user_query)
        
        print("\n--- Retrieving Context & Analysing... ---")
        for doc, score in zip(docs, scores):
            print(f"- {doc} (Score: {score:.4f})")
        
        print("\n--- LLM Analysis ---")
        print(analysis)
        print("\n---------------------\n")
        

def main():

    all_domain_ip_relations = []
    all_get_requests = []
    all_post_requests = []

    print("\nüöÄPart I: Semantic Matching\n")
    print("\n‚öôÔ∏èProcessing Input Files: ...\n")
    time.sleep(2)
    for pcap_file_path in pcap_file_paths:

        print(f"‚öôÔ∏èProcessing file: {pcap_file_path}")
        domain_ip_relations, get_requests, post_requests = extract_domain_ip_and_http_requests(pcap_file_path)
        all_domain_ip_relations.extend(domain_ip_relations)
        all_get_requests.extend(get_requests)
        all_post_requests.extend(post_requests)

    print("\n‚öóÔ∏èExtracting domain-to-IP relationships and HTTP GET requests...")


    domain_to_urls = group_urls_by_domain(all_get_requests, all_post_requests)
    #print(f"Extracted Domain-to-IP Relationships: {all_domain_ip_relations}")
    #print(f"\nExtracted HTTP GET Requests: {all_get_requests}")
    #print(f"\nExtracted HTTP POST Requests: {all_get_requests}")
    
    print("\nüî¨Detecting anomalies...")
    print("\nüîéSemantic Searches in Porgess...")
    anomalies = detect_anomalies_rag(all_get_requests, all_post_requests)
    print(f"\n")
    print_anomalies(anomalies)

    #time.sleep(2)
    print("\nüöÄPart II: RAG-LLM\n")
    print("üéõÔ∏èLooping in LLM...")
    time.sleep(2)
    query_llm_interface()

if __name__ == "__main__":
    main()




